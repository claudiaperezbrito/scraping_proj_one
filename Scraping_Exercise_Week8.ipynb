{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Exercise, Week 8\n",
    "\n",
    "This notebook is all one exercise, and all one script, even though it is broken into separate Jupyter cells. Follow the comments in each cell, in order from top to bottom, to complete the exercise.\n",
    "\n",
    "**Make sure each cell runs successfully before moving to the next cell.** Press Shift-Return inside a cell to test it.\n",
    "\n",
    "TIPS: \n",
    "* Do not change the lines I have written, unless: \n",
    "   * There's a question mark where I've told you to enter a URL. \n",
    "   * There's a return line - you'll need to add whatever the function returns, e.g. the variable for a list \n",
    "* Using the lines and variable names as I have written them will work. Changing them will not help. \n",
    "* Where I have left blank lines, you may add as many lines as you need. The number of blank lines is not important.\n",
    "* Un-comment the commented print statement at the end of the cell to run and test the code in that cell. Then return the hash mark to comment it out again. (The first and last cells do not have a print statement.) You can add print statements any time you think they might help you, but usually we remove them from the final code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have not pip installed BeautifulSoup in your Jupyter virtualenv, you will get \n",
    "# this error when you Shift-Return inside this cell:\n",
    "# ImportError: cannot import name 'BeautifulSoup'\n",
    "\n",
    "# import everything that needs to be imported (done; do not change)\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/name/nm0037410/?ref_=tt_cl_t1', '/name/nm0049079/?ref_=tt_cl_t2', '/name/nm0000113/?ref_=tt_cl_t3', '/name/nm0000332/?ref_=tt_cl_t4', '/name/nm0160988/?ref_=tt_cl_t5', '/name/nm0181691/?ref_=tt_cl_t6', '/name/nm0001103/?ref_=tt_cl_t7', '/name/nm0202966/?ref_=tt_cl_t8', '/name/nm0222643/?ref_=tt_cl_t9', '/name/nm0000369/?ref_=tt_cl_t10', '/name/nm0261170/?ref_=tt_cl_t11', '/name/nm1096532/?ref_=tt_cl_t12', '/name/nm0272961/?ref_=tt_cl_t13', '/name/nm0001209/?ref_=tt_cl_t14', '/name/nm0284567/?ref_=tt_cl_t15']\n",
      "['/name/nm0037410/?ref_=tt_cl_t1', '/name/nm0049079/?ref_=tt_cl_t2', '/name/nm0000113/?ref_=tt_cl_t3', '/name/nm0000332/?ref_=tt_cl_t4', '/name/nm0160988/?ref_=tt_cl_t5', '/name/nm0181691/?ref_=tt_cl_t6', '/name/nm0001103/?ref_=tt_cl_t7', '/name/nm0202966/?ref_=tt_cl_t8', '/name/nm0222643/?ref_=tt_cl_t9', '/name/nm0000369/?ref_=tt_cl_t10', '/name/nm0261170/?ref_=tt_cl_t11', '/name/nm1096532/?ref_=tt_cl_t12', '/name/nm0272961/?ref_=tt_cl_t13', '/name/nm0001209/?ref_=tt_cl_t14', '/name/nm0284567/?ref_=tt_cl_t15']\n"
     ]
    }
   ],
   "source": [
    "#\"\" write a function (ONE function) to scrape all actors' URLs from one movie page at IMDb.com\n",
    "# store them in a list\n",
    "# return the list\n",
    "\n",
    "# provide the movie page's complete URL\n",
    "my_url = \"https://www.imdb.com/title/tt0375679/\"\n",
    "\n",
    "def get_urls_from_single_page(movie_url):\n",
    "    # open page and create bsObj - 2 lines \n",
    "    html = urlopen(movie_url)\n",
    "    bs = BeautifulSoup(html, \"html.parser\")\n",
    "    # create empty list to hold all URLs\n",
    "    names_list = []\n",
    "    # get the URLs and write them one-by-one into the list\n",
    "    cast = bs.find(\"table\",{\"class\":\"cast_list\"})\n",
    "    for table in cast.findAll(\"tr\"):\n",
    "        for tr in table.findAll(\"td\", {\"class\":\"\"}):\n",
    "            for td in tr.findAll(\"a\"):\n",
    "                names_list.append(td.get('href'))\n",
    "                #print(td.get('href')) #this is to test code is working\n",
    "    # return the completed list of URLs - end of function\n",
    "    return names_list\n",
    "\n",
    "# call the function\n",
    "url_list = get_urls_from_single_page(my_url)\n",
    "# the following should print a list containing the partial URLs for all actors \n",
    "print(url_list)\n",
    "#print(names_list) #this is to test code is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sandra Bullock', 'July 26,1964')]\n"
     ]
    }
   ],
   "source": [
    "# write a function (ONE function) to open one actor page at IMDb.com and scrape:\n",
    "# - actor's full name\n",
    "# - actor's birthday - month, day and year\n",
    "# store just those two items in a list\n",
    "# return the list\n",
    "\n",
    "# provide one actor page's partial URL (like the ones in url_list)\n",
    "test_url = \"/name/nm0000113/?ref_=ttfc_fc_cl_t3\"\n",
    "\n",
    "def scrape_one_actor(actor_url):\n",
    "    # open page and create bsObj - 2 lines \n",
    "    html = urlopen(\"https://www.imdb.com/name/nm0000113/?ref_=ttfc_fc_cl_t3\")\n",
    "    bs = BeautifulSoup(html, \"html.parser\")\n",
    "    # get actor name\n",
    "    actor_name = bs.find('span', {'class': 'itemprop'})\n",
    "    #print(actor_name.get_text()) \n",
    "    # get actor birthday - this is tricky - HINT: use try/except\n",
    "    try:\n",
    "        #actor_bday = bs.find('time')\n",
    "        actor_bday = bs.find('div',{'id': 'name-born-info'}).findAll('a')[0]\n",
    "        actor_byear = bs.find('div',{'id': 'name-born-info'}).findAll('a')[1]\n",
    "        actor_full_bday = (actor_name.get_text(),actor_bday.get_text() + ',' + actor_byear.get_text())\n",
    "        #print(actor_full_bday) #this is to test code is working\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "        print('N/A')\n",
    "    # put them in a list and return the list - end of function \n",
    "    actor_birthdate = []\n",
    "    actor_birthdate.append(actor_full_bday)\n",
    "    \n",
    "    #print(actor_birthdate) #this is to test code is working\n",
    "    return actor_birthdate\n",
    "# call the function\n",
    "actor_details = scrape_one_actor(test_url)\n",
    "# the following should print a list that contains two items: name and birthday\n",
    "print(actor_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write a function (ONE function) to send all the actor URLs through the scrape_one_actor() function above \n",
    "# and here, in the same function, write out the following to a CSV file:\n",
    "# [ actor's name, birthday, partial URL ]\n",
    "# in top line of the CSV, write the following column headings: actor, birthday, url\n",
    "\n",
    "def write_csv(url_list):\n",
    "    # open new file for writing - name it actors.csv \n",
    "    csvfile = open('actors.csv', 'w', newline='', encoding='utf-8')\n",
    "    # make a Python CSV writer object -\n",
    "    c = csv.writer(csvfile)\n",
    "    # write the column headings row \n",
    "    c.writerow(['actor', 'birthday', 'url'])\n",
    "    # loop through the urls to run the scrape_one_actor() function for each url \n",
    "    # insert a 1-second time delay inside the loop \n",
    "   \n",
    "    for actor_name in url_list:\n",
    "        actor = scrape_one_actor(actor_name)\n",
    "        c.writerow(actor)\n",
    "\n",
    "    \n",
    "    # close the file - end of function \n",
    "    csvfile.close()\n",
    "    # function does not need a return - but the next line prevents an error in Jupyter, so keep it \n",
    "    return None\n",
    "\n",
    "# run the function - this should create a .csv file \n",
    "write_csv(url_list)\n",
    "#print(url_list) #this is jsut to check\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've come to the end of the exercise. \n",
    "\n",
    "Check your Jupyter directory for a file named actors.csv - if you have a line of headings and 15 lines with actor information, you've done it all. If you have errors above, though, the .csv was not written.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
